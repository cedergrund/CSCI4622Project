{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Read ReadMe for instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgboost\n",
    "import re\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/6_8zt65542g2nv99zls1h_nr0000gn/T/ipykernel_25825/3125798192.py:1: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  accepted_df = pd.read_csv(\"datasets/accepted_dataset.csv\")\n"
     ]
    }
   ],
   "source": [
    "accepted_df = pd.read_csv(\"datasets/accepted_dataset.csv\")\n",
    "rejected_df = pd.read_csv(\"datasets/rejected_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>pymnt_plan</th>\n",
       "      <th>url</th>\n",
       "      <th>desc</th>\n",
       "      <th>purpose</th>\n",
       "      <th>title</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>earliest_cr_line</th>\n",
       "      <th>fico_range_low</th>\n",
       "      <th>fico_range_high</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>mths_since_last_delinq</th>\n",
       "      <th>mths_since_last_record</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>out_prncp</th>\n",
       "      <th>out_prncp_inv</th>\n",
       "      <th>total_pymnt</th>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <th>total_rec_int</th>\n",
       "      <th>total_rec_late_fee</th>\n",
       "      <th>recoveries</th>\n",
       "      <th>collection_recovery_fee</th>\n",
       "      <th>last_pymnt_d</th>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <th>next_pymnt_d</th>\n",
       "      <th>last_credit_pull_d</th>\n",
       "      <th>last_fico_range_high</th>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>mths_since_last_major_derog</th>\n",
       "      <th>policy_code</th>\n",
       "      <th>application_type</th>\n",
       "      <th>annual_inc_joint</th>\n",
       "      <th>dti_joint</th>\n",
       "      <th>verification_status_joint</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>tot_coll_amt</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>open_acc_6m</th>\n",
       "      <th>open_act_il</th>\n",
       "      <th>open_il_12m</th>\n",
       "      <th>open_il_24m</th>\n",
       "      <th>mths_since_rcnt_il</th>\n",
       "      <th>total_bal_il</th>\n",
       "      <th>il_util</th>\n",
       "      <th>open_rv_12m</th>\n",
       "      <th>open_rv_24m</th>\n",
       "      <th>max_bal_bc</th>\n",
       "      <th>all_util</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>inq_fi</th>\n",
       "      <th>total_cu_tl</th>\n",
       "      <th>inq_last_12m</th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>mo_sin_old_il_acct</th>\n",
       "      <th>mo_sin_old_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_tl</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>mths_since_recent_bc</th>\n",
       "      <th>mths_since_recent_bc_dlq</th>\n",
       "      <th>mths_since_recent_inq</th>\n",
       "      <th>mths_since_recent_revol_delinq</th>\n",
       "      <th>num_accts_ever_120_pd</th>\n",
       "      <th>num_actv_bc_tl</th>\n",
       "      <th>num_actv_rev_tl</th>\n",
       "      <th>num_bc_sats</th>\n",
       "      <th>num_bc_tl</th>\n",
       "      <th>num_il_tl</th>\n",
       "      <th>num_op_rev_tl</th>\n",
       "      <th>num_rev_accts</th>\n",
       "      <th>num_rev_tl_bal_gt_0</th>\n",
       "      <th>num_sats</th>\n",
       "      <th>num_tl_120dpd_2m</th>\n",
       "      <th>num_tl_30dpd</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>revol_bal_joint</th>\n",
       "      <th>sec_app_fico_range_low</th>\n",
       "      <th>sec_app_fico_range_high</th>\n",
       "      <th>sec_app_earliest_cr_line</th>\n",
       "      <th>sec_app_inq_last_6mths</th>\n",
       "      <th>sec_app_mort_acc</th>\n",
       "      <th>sec_app_open_acc</th>\n",
       "      <th>sec_app_revol_util</th>\n",
       "      <th>sec_app_open_act_il</th>\n",
       "      <th>sec_app_num_rev_accts</th>\n",
       "      <th>sec_app_chargeoff_within_12_mths</th>\n",
       "      <th>sec_app_collections_12_mths_ex_med</th>\n",
       "      <th>sec_app_mths_since_last_major_derog</th>\n",
       "      <th>hardship_flag</th>\n",
       "      <th>hardship_type</th>\n",
       "      <th>hardship_reason</th>\n",
       "      <th>hardship_status</th>\n",
       "      <th>deferral_term</th>\n",
       "      <th>hardship_amount</th>\n",
       "      <th>hardship_start_date</th>\n",
       "      <th>hardship_end_date</th>\n",
       "      <th>payment_plan_start_date</th>\n",
       "      <th>hardship_length</th>\n",
       "      <th>hardship_dpd</th>\n",
       "      <th>hardship_loan_status</th>\n",
       "      <th>orig_projected_additional_accrued_interest</th>\n",
       "      <th>hardship_payoff_balance_amount</th>\n",
       "      <th>hardship_last_payment_amount</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>debt_settlement_flag_date</th>\n",
       "      <th>settlement_status</th>\n",
       "      <th>settlement_date</th>\n",
       "      <th>settlement_amount</th>\n",
       "      <th>settlement_percentage</th>\n",
       "      <th>settlement_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68407277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.99</td>\n",
       "      <td>123.03</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>leadman</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Dec-2015</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>n</td>\n",
       "      <td>https://lendingclub.com/browse/loanDetail.acti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>190xx</td>\n",
       "      <td>PA</td>\n",
       "      <td>5.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Aug-2003</td>\n",
       "      <td>675.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2765.0</td>\n",
       "      <td>29.7</td>\n",
       "      <td>13.0</td>\n",
       "      <td>w</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4421.723917</td>\n",
       "      <td>4421.72</td>\n",
       "      <td>3600.00</td>\n",
       "      <td>821.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jan-2019</td>\n",
       "      <td>122.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-2019</td>\n",
       "      <td>564.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>722.0</td>\n",
       "      <td>144904.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4981.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>722.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>9300.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20701.0</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>76.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178050.0</td>\n",
       "      <td>7746.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>13734.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68355089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>11.99</td>\n",
       "      <td>820.28</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Dec-2015</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>n</td>\n",
       "      <td>https://lendingclub.com/browse/loanDetail.acti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>small_business</td>\n",
       "      <td>Business</td>\n",
       "      <td>577xx</td>\n",
       "      <td>SD</td>\n",
       "      <td>16.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dec-1999</td>\n",
       "      <td>715.0</td>\n",
       "      <td>719.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21470.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>w</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25679.660000</td>\n",
       "      <td>25679.66</td>\n",
       "      <td>24700.00</td>\n",
       "      <td>979.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun-2016</td>\n",
       "      <td>926.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-2019</td>\n",
       "      <td>699.0</td>\n",
       "      <td>695.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>204396.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18005.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6472.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>111800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9733.0</td>\n",
       "      <td>57830.0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>97.4</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>314017.0</td>\n",
       "      <td>39475.0</td>\n",
       "      <td>79300.0</td>\n",
       "      <td>24667.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68341763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>10.78</td>\n",
       "      <td>432.66</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>truck driver</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Dec-2015</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>n</td>\n",
       "      <td>https://lendingclub.com/browse/loanDetail.acti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>home_improvement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>605xx</td>\n",
       "      <td>IL</td>\n",
       "      <td>10.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Aug-2000</td>\n",
       "      <td>695.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7869.0</td>\n",
       "      <td>56.2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>w</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22705.924294</td>\n",
       "      <td>22705.92</td>\n",
       "      <td>20000.00</td>\n",
       "      <td>2705.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun-2017</td>\n",
       "      <td>15813.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-2019</td>\n",
       "      <td>704.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Joint App</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>13.85</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189699.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10827.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2081.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31617.0</td>\n",
       "      <td>2737.0</td>\n",
       "      <td>55.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>218418.0</td>\n",
       "      <td>18696.0</td>\n",
       "      <td>6200.0</td>\n",
       "      <td>14877.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66310712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>14.85</td>\n",
       "      <td>829.90</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>Information Systems Officer</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Dec-2015</td>\n",
       "      <td>Current</td>\n",
       "      <td>n</td>\n",
       "      <td>https://lendingclub.com/browse/loanDetail.acti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>076xx</td>\n",
       "      <td>NJ</td>\n",
       "      <td>17.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sep-2008</td>\n",
       "      <td>785.0</td>\n",
       "      <td>789.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7802.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>17.0</td>\n",
       "      <td>w</td>\n",
       "      <td>15897.65</td>\n",
       "      <td>15897.65</td>\n",
       "      <td>31464.010000</td>\n",
       "      <td>31464.01</td>\n",
       "      <td>19102.35</td>\n",
       "      <td>12361.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb-2019</td>\n",
       "      <td>829.90</td>\n",
       "      <td>Apr-2019</td>\n",
       "      <td>Mar-2019</td>\n",
       "      <td>679.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>301500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>12609.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6987.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>67300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23192.0</td>\n",
       "      <td>54962.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>381215.0</td>\n",
       "      <td>52226.0</td>\n",
       "      <td>62500.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68476807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>22.45</td>\n",
       "      <td>289.91</td>\n",
       "      <td>F</td>\n",
       "      <td>F1</td>\n",
       "      <td>Contract Specialist</td>\n",
       "      <td>3 years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>104433.0</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Dec-2015</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>n</td>\n",
       "      <td>https://lendingclub.com/browse/loanDetail.acti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>major_purchase</td>\n",
       "      <td>Major purchase</td>\n",
       "      <td>174xx</td>\n",
       "      <td>PA</td>\n",
       "      <td>25.37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jun-1998</td>\n",
       "      <td>695.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21929.0</td>\n",
       "      <td>64.5</td>\n",
       "      <td>35.0</td>\n",
       "      <td>w</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11740.500000</td>\n",
       "      <td>11740.50</td>\n",
       "      <td>10400.00</td>\n",
       "      <td>1340.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jul-2016</td>\n",
       "      <td>10128.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-2018</td>\n",
       "      <td>704.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>331730.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>73839.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9702.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>34000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27644.0</td>\n",
       "      <td>4567.0</td>\n",
       "      <td>77.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96.6</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>439570.0</td>\n",
       "      <td>95768.0</td>\n",
       "      <td>20300.0</td>\n",
       "      <td>88097.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "0  68407277        NaN     3600.0       3600.0           3600.0   36 months   \n",
       "1  68355089        NaN    24700.0      24700.0          24700.0   36 months   \n",
       "2  68341763        NaN    20000.0      20000.0          20000.0   60 months   \n",
       "3  66310712        NaN    35000.0      35000.0          35000.0   60 months   \n",
       "4  68476807        NaN    10400.0      10400.0          10400.0   60 months   \n",
       "\n",
       "   int_rate  installment grade sub_grade                    emp_title  \\\n",
       "0     13.99       123.03     C        C4                      leadman   \n",
       "1     11.99       820.28     C        C1                     Engineer   \n",
       "2     10.78       432.66     B        B4                 truck driver   \n",
       "3     14.85       829.90     C        C5  Information Systems Officer   \n",
       "4     22.45       289.91     F        F1          Contract Specialist   \n",
       "\n",
       "  emp_length home_ownership  annual_inc verification_status   issue_d  \\\n",
       "0  10+ years       MORTGAGE     55000.0        Not Verified  Dec-2015   \n",
       "1  10+ years       MORTGAGE     65000.0        Not Verified  Dec-2015   \n",
       "2  10+ years       MORTGAGE     63000.0        Not Verified  Dec-2015   \n",
       "3  10+ years       MORTGAGE    110000.0     Source Verified  Dec-2015   \n",
       "4    3 years       MORTGAGE    104433.0     Source Verified  Dec-2015   \n",
       "\n",
       "  loan_status pymnt_plan                                                url  \\\n",
       "0  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...   \n",
       "1  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...   \n",
       "2  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...   \n",
       "3     Current          n  https://lendingclub.com/browse/loanDetail.acti...   \n",
       "4  Fully Paid          n  https://lendingclub.com/browse/loanDetail.acti...   \n",
       "\n",
       "  desc             purpose               title zip_code addr_state    dti  \\\n",
       "0  NaN  debt_consolidation  Debt consolidation    190xx         PA   5.91   \n",
       "1  NaN      small_business            Business    577xx         SD  16.06   \n",
       "2  NaN    home_improvement                 NaN    605xx         IL  10.78   \n",
       "3  NaN  debt_consolidation  Debt consolidation    076xx         NJ  17.06   \n",
       "4  NaN      major_purchase      Major purchase    174xx         PA  25.37   \n",
       "\n",
       "   delinq_2yrs earliest_cr_line  fico_range_low  fico_range_high  \\\n",
       "0          0.0         Aug-2003           675.0            679.0   \n",
       "1          1.0         Dec-1999           715.0            719.0   \n",
       "2          0.0         Aug-2000           695.0            699.0   \n",
       "3          0.0         Sep-2008           785.0            789.0   \n",
       "4          1.0         Jun-1998           695.0            699.0   \n",
       "\n",
       "   inq_last_6mths  mths_since_last_delinq  mths_since_last_record  open_acc  \\\n",
       "0             1.0                    30.0                     NaN       7.0   \n",
       "1             4.0                     6.0                     NaN      22.0   \n",
       "2             0.0                     NaN                     NaN       6.0   \n",
       "3             0.0                     NaN                     NaN      13.0   \n",
       "4             3.0                    12.0                     NaN      12.0   \n",
       "\n",
       "   pub_rec  revol_bal  revol_util  total_acc initial_list_status  out_prncp  \\\n",
       "0      0.0     2765.0        29.7       13.0                   w       0.00   \n",
       "1      0.0    21470.0        19.2       38.0                   w       0.00   \n",
       "2      0.0     7869.0        56.2       18.0                   w       0.00   \n",
       "3      0.0     7802.0        11.6       17.0                   w   15897.65   \n",
       "4      0.0    21929.0        64.5       35.0                   w       0.00   \n",
       "\n",
       "   out_prncp_inv   total_pymnt  total_pymnt_inv  total_rec_prncp  \\\n",
       "0           0.00   4421.723917          4421.72          3600.00   \n",
       "1           0.00  25679.660000         25679.66         24700.00   \n",
       "2           0.00  22705.924294         22705.92         20000.00   \n",
       "3       15897.65  31464.010000         31464.01         19102.35   \n",
       "4           0.00  11740.500000         11740.50         10400.00   \n",
       "\n",
       "   total_rec_int  total_rec_late_fee  recoveries  collection_recovery_fee  \\\n",
       "0         821.72                 0.0         0.0                      0.0   \n",
       "1         979.66                 0.0         0.0                      0.0   \n",
       "2        2705.92                 0.0         0.0                      0.0   \n",
       "3       12361.66                 0.0         0.0                      0.0   \n",
       "4        1340.50                 0.0         0.0                      0.0   \n",
       "\n",
       "  last_pymnt_d  last_pymnt_amnt next_pymnt_d last_credit_pull_d  \\\n",
       "0     Jan-2019           122.67          NaN           Mar-2019   \n",
       "1     Jun-2016           926.35          NaN           Mar-2019   \n",
       "2     Jun-2017         15813.30          NaN           Mar-2019   \n",
       "3     Feb-2019           829.90     Apr-2019           Mar-2019   \n",
       "4     Jul-2016         10128.96          NaN           Mar-2018   \n",
       "\n",
       "   last_fico_range_high  last_fico_range_low  collections_12_mths_ex_med  \\\n",
       "0                 564.0                560.0                         0.0   \n",
       "1                 699.0                695.0                         0.0   \n",
       "2                 704.0                700.0                         0.0   \n",
       "3                 679.0                675.0                         0.0   \n",
       "4                 704.0                700.0                         0.0   \n",
       "\n",
       "   mths_since_last_major_derog  policy_code application_type  \\\n",
       "0                         30.0          1.0       Individual   \n",
       "1                          NaN          1.0       Individual   \n",
       "2                          NaN          1.0        Joint App   \n",
       "3                          NaN          1.0       Individual   \n",
       "4                          NaN          1.0       Individual   \n",
       "\n",
       "   annual_inc_joint  dti_joint verification_status_joint  acc_now_delinq  \\\n",
       "0               NaN        NaN                       NaN             0.0   \n",
       "1               NaN        NaN                       NaN             0.0   \n",
       "2           71000.0      13.85              Not Verified             0.0   \n",
       "3               NaN        NaN                       NaN             0.0   \n",
       "4               NaN        NaN                       NaN             0.0   \n",
       "\n",
       "   tot_coll_amt  tot_cur_bal  open_acc_6m  open_act_il  open_il_12m  \\\n",
       "0         722.0     144904.0          2.0          2.0          0.0   \n",
       "1           0.0     204396.0          1.0          1.0          0.0   \n",
       "2           0.0     189699.0          0.0          1.0          0.0   \n",
       "3           0.0     301500.0          1.0          1.0          0.0   \n",
       "4           0.0     331730.0          1.0          3.0          0.0   \n",
       "\n",
       "   open_il_24m  mths_since_rcnt_il  total_bal_il  il_util  open_rv_12m  \\\n",
       "0          1.0                21.0        4981.0     36.0          3.0   \n",
       "1          1.0                19.0       18005.0     73.0          2.0   \n",
       "2          4.0                19.0       10827.0     73.0          0.0   \n",
       "3          1.0                23.0       12609.0     70.0          1.0   \n",
       "4          3.0                14.0       73839.0     84.0          4.0   \n",
       "\n",
       "   open_rv_24m  max_bal_bc  all_util  total_rev_hi_lim  inq_fi  total_cu_tl  \\\n",
       "0          3.0       722.0      34.0            9300.0     3.0          1.0   \n",
       "1          3.0      6472.0      29.0          111800.0     0.0          0.0   \n",
       "2          2.0      2081.0      65.0           14000.0     2.0          5.0   \n",
       "3          1.0      6987.0      45.0           67300.0     0.0          1.0   \n",
       "4          7.0      9702.0      78.0           34000.0     2.0          1.0   \n",
       "\n",
       "   inq_last_12m  acc_open_past_24mths  avg_cur_bal  bc_open_to_buy  bc_util  \\\n",
       "0           4.0                   4.0      20701.0          1506.0     37.2   \n",
       "1           6.0                   4.0       9733.0         57830.0     27.1   \n",
       "2           1.0                   6.0      31617.0          2737.0     55.9   \n",
       "3           0.0                   2.0      23192.0         54962.0     12.1   \n",
       "4           3.0                  10.0      27644.0          4567.0     77.5   \n",
       "\n",
       "   chargeoff_within_12_mths  delinq_amnt  mo_sin_old_il_acct  \\\n",
       "0                       0.0          0.0               148.0   \n",
       "1                       0.0          0.0               113.0   \n",
       "2                       0.0          0.0               125.0   \n",
       "3                       0.0          0.0                36.0   \n",
       "4                       0.0          0.0               128.0   \n",
       "\n",
       "   mo_sin_old_rev_tl_op  mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  mort_acc  \\\n",
       "0                 128.0                    3.0             3.0       1.0   \n",
       "1                 192.0                    2.0             2.0       4.0   \n",
       "2                 184.0                   14.0            14.0       5.0   \n",
       "3                  87.0                    2.0             2.0       1.0   \n",
       "4                 210.0                    4.0             4.0       6.0   \n",
       "\n",
       "   mths_since_recent_bc  mths_since_recent_bc_dlq  mths_since_recent_inq  \\\n",
       "0                   4.0                      69.0                    4.0   \n",
       "1                   2.0                       NaN                    0.0   \n",
       "2                 101.0                       NaN                   10.0   \n",
       "3                   2.0                       NaN                    NaN   \n",
       "4                   4.0                      12.0                    1.0   \n",
       "\n",
       "   mths_since_recent_revol_delinq  num_accts_ever_120_pd  num_actv_bc_tl  \\\n",
       "0                            69.0                    2.0             2.0   \n",
       "1                             6.0                    0.0             5.0   \n",
       "2                             NaN                    0.0             2.0   \n",
       "3                             NaN                    0.0             4.0   \n",
       "4                            12.0                    0.0             4.0   \n",
       "\n",
       "   num_actv_rev_tl  num_bc_sats  num_bc_tl  num_il_tl  num_op_rev_tl  \\\n",
       "0              4.0          2.0        5.0        3.0            4.0   \n",
       "1              5.0         13.0       17.0        6.0           20.0   \n",
       "2              3.0          2.0        4.0        6.0            4.0   \n",
       "3              5.0          8.0       10.0        2.0           10.0   \n",
       "4              6.0          5.0        9.0       10.0            7.0   \n",
       "\n",
       "   num_rev_accts  num_rev_tl_bal_gt_0  num_sats  num_tl_120dpd_2m  \\\n",
       "0            9.0                  4.0       7.0               0.0   \n",
       "1           27.0                  5.0      22.0               0.0   \n",
       "2            7.0                  3.0       6.0               0.0   \n",
       "3           13.0                  5.0      13.0               0.0   \n",
       "4           19.0                  6.0      12.0               0.0   \n",
       "\n",
       "   num_tl_30dpd  num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  \\\n",
       "0           0.0                 0.0                 3.0            76.9   \n",
       "1           0.0                 0.0                 2.0            97.4   \n",
       "2           0.0                 0.0                 0.0           100.0   \n",
       "3           0.0                 0.0                 1.0           100.0   \n",
       "4           0.0                 0.0                 4.0            96.6   \n",
       "\n",
       "   percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  tot_hi_cred_lim  \\\n",
       "0               0.0                   0.0        0.0         178050.0   \n",
       "1               7.7                   0.0        0.0         314017.0   \n",
       "2              50.0                   0.0        0.0         218418.0   \n",
       "3               0.0                   0.0        0.0         381215.0   \n",
       "4              60.0                   0.0        0.0         439570.0   \n",
       "\n",
       "   total_bal_ex_mort  total_bc_limit  total_il_high_credit_limit  \\\n",
       "0             7746.0          2400.0                     13734.0   \n",
       "1            39475.0         79300.0                     24667.0   \n",
       "2            18696.0          6200.0                     14877.0   \n",
       "3            52226.0         62500.0                     18000.0   \n",
       "4            95768.0         20300.0                     88097.0   \n",
       "\n",
       "   revol_bal_joint  sec_app_fico_range_low  sec_app_fico_range_high  \\\n",
       "0              NaN                     NaN                      NaN   \n",
       "1              NaN                     NaN                      NaN   \n",
       "2              NaN                     NaN                      NaN   \n",
       "3              NaN                     NaN                      NaN   \n",
       "4              NaN                     NaN                      NaN   \n",
       "\n",
       "  sec_app_earliest_cr_line  sec_app_inq_last_6mths  sec_app_mort_acc  \\\n",
       "0                      NaN                     NaN               NaN   \n",
       "1                      NaN                     NaN               NaN   \n",
       "2                      NaN                     NaN               NaN   \n",
       "3                      NaN                     NaN               NaN   \n",
       "4                      NaN                     NaN               NaN   \n",
       "\n",
       "   sec_app_open_acc  sec_app_revol_util  sec_app_open_act_il  \\\n",
       "0               NaN                 NaN                  NaN   \n",
       "1               NaN                 NaN                  NaN   \n",
       "2               NaN                 NaN                  NaN   \n",
       "3               NaN                 NaN                  NaN   \n",
       "4               NaN                 NaN                  NaN   \n",
       "\n",
       "   sec_app_num_rev_accts  sec_app_chargeoff_within_12_mths  \\\n",
       "0                    NaN                               NaN   \n",
       "1                    NaN                               NaN   \n",
       "2                    NaN                               NaN   \n",
       "3                    NaN                               NaN   \n",
       "4                    NaN                               NaN   \n",
       "\n",
       "   sec_app_collections_12_mths_ex_med  sec_app_mths_since_last_major_derog  \\\n",
       "0                                 NaN                                  NaN   \n",
       "1                                 NaN                                  NaN   \n",
       "2                                 NaN                                  NaN   \n",
       "3                                 NaN                                  NaN   \n",
       "4                                 NaN                                  NaN   \n",
       "\n",
       "  hardship_flag hardship_type hardship_reason hardship_status  deferral_term  \\\n",
       "0             N           NaN             NaN             NaN            NaN   \n",
       "1             N           NaN             NaN             NaN            NaN   \n",
       "2             N           NaN             NaN             NaN            NaN   \n",
       "3             N           NaN             NaN             NaN            NaN   \n",
       "4             N           NaN             NaN             NaN            NaN   \n",
       "\n",
       "   hardship_amount hardship_start_date hardship_end_date  \\\n",
       "0              NaN                 NaN               NaN   \n",
       "1              NaN                 NaN               NaN   \n",
       "2              NaN                 NaN               NaN   \n",
       "3              NaN                 NaN               NaN   \n",
       "4              NaN                 NaN               NaN   \n",
       "\n",
       "  payment_plan_start_date  hardship_length  hardship_dpd hardship_loan_status  \\\n",
       "0                     NaN              NaN           NaN                  NaN   \n",
       "1                     NaN              NaN           NaN                  NaN   \n",
       "2                     NaN              NaN           NaN                  NaN   \n",
       "3                     NaN              NaN           NaN                  NaN   \n",
       "4                     NaN              NaN           NaN                  NaN   \n",
       "\n",
       "   orig_projected_additional_accrued_interest  hardship_payoff_balance_amount  \\\n",
       "0                                         NaN                             NaN   \n",
       "1                                         NaN                             NaN   \n",
       "2                                         NaN                             NaN   \n",
       "3                                         NaN                             NaN   \n",
       "4                                         NaN                             NaN   \n",
       "\n",
       "   hardship_last_payment_amount disbursement_method debt_settlement_flag  \\\n",
       "0                           NaN                Cash                    N   \n",
       "1                           NaN                Cash                    N   \n",
       "2                           NaN                Cash                    N   \n",
       "3                           NaN                Cash                    N   \n",
       "4                           NaN                Cash                    N   \n",
       "\n",
       "  debt_settlement_flag_date settlement_status settlement_date  \\\n",
       "0                       NaN               NaN             NaN   \n",
       "1                       NaN               NaN             NaN   \n",
       "2                       NaN               NaN             NaN   \n",
       "3                       NaN               NaN             NaN   \n",
       "4                       NaN               NaN             NaN   \n",
       "\n",
       "   settlement_amount  settlement_percentage  settlement_term  \n",
       "0                NaN                    NaN              NaN  \n",
       "1                NaN                    NaN              NaN  \n",
       "2                NaN                    NaN              NaN  \n",
       "3                NaN                    NaN              NaN  \n",
       "4                NaN                    NaN              NaN  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accepted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount Requested</th>\n",
       "      <th>Application Date</th>\n",
       "      <th>Loan Title</th>\n",
       "      <th>Risk_Score</th>\n",
       "      <th>Debt-To-Income Ratio</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>State</th>\n",
       "      <th>Employment Length</th>\n",
       "      <th>Policy Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>2007-05-26</td>\n",
       "      <td>Wedding Covered but No Honeymoon</td>\n",
       "      <td>693.0</td>\n",
       "      <td>10%</td>\n",
       "      <td>481xx</td>\n",
       "      <td>NM</td>\n",
       "      <td>4 years</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>2007-05-26</td>\n",
       "      <td>Consolidating Debt</td>\n",
       "      <td>703.0</td>\n",
       "      <td>10%</td>\n",
       "      <td>010xx</td>\n",
       "      <td>MA</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11000.0</td>\n",
       "      <td>2007-05-27</td>\n",
       "      <td>Want to consolidate my debt</td>\n",
       "      <td>715.0</td>\n",
       "      <td>10%</td>\n",
       "      <td>212xx</td>\n",
       "      <td>MD</td>\n",
       "      <td>1 year</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>2007-05-27</td>\n",
       "      <td>waksman</td>\n",
       "      <td>698.0</td>\n",
       "      <td>38.64%</td>\n",
       "      <td>017xx</td>\n",
       "      <td>MA</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>2007-05-27</td>\n",
       "      <td>mdrigo</td>\n",
       "      <td>509.0</td>\n",
       "      <td>9.43%</td>\n",
       "      <td>209xx</td>\n",
       "      <td>MD</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amount Requested Application Date                        Loan Title  \\\n",
       "0            1000.0       2007-05-26  Wedding Covered but No Honeymoon   \n",
       "1            1000.0       2007-05-26                Consolidating Debt   \n",
       "2           11000.0       2007-05-27       Want to consolidate my debt   \n",
       "3            6000.0       2007-05-27                           waksman   \n",
       "4            1500.0       2007-05-27                            mdrigo   \n",
       "\n",
       "   Risk_Score Debt-To-Income Ratio Zip Code State Employment Length  \\\n",
       "0       693.0                  10%    481xx    NM           4 years   \n",
       "1       703.0                  10%    010xx    MA          < 1 year   \n",
       "2       715.0                  10%    212xx    MD            1 year   \n",
       "3       698.0               38.64%    017xx    MA          < 1 year   \n",
       "4       509.0                9.43%    209xx    MD          < 1 year   \n",
       "\n",
       "   Policy Code  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rejected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',\n",
       "       'term', 'int_rate', 'installment', 'grade', 'sub_grade',\n",
       "       'emp_title', 'emp_length', 'home_ownership', 'annual_inc',\n",
       "       'verification_status', 'issue_d', 'loan_status', 'pymnt_plan',\n",
       "       'url', 'desc', 'purpose', 'title', 'zip_code', 'addr_state', 'dti',\n",
       "       'delinq_2yrs', 'earliest_cr_line', 'fico_range_low',\n",
       "       'fico_range_high', 'inq_last_6mths', 'mths_since_last_delinq',\n",
       "       'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal',\n",
       "       'revol_util', 'total_acc', 'initial_list_status', 'out_prncp',\n",
       "       'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv',\n",
       "       'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee',\n",
       "       'recoveries', 'collection_recovery_fee', 'last_pymnt_d',\n",
       "       'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d',\n",
       "       'last_fico_range_high', 'last_fico_range_low',\n",
       "       'collections_12_mths_ex_med', 'mths_since_last_major_derog',\n",
       "       'policy_code', 'application_type', 'annual_inc_joint', 'dti_joint',\n",
       "       'verification_status_joint', 'acc_now_delinq', 'tot_coll_amt',\n",
       "       'tot_cur_bal', 'open_acc_6m', 'open_act_il', 'open_il_12m',\n",
       "       'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util',\n",
       "       'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util',\n",
       "       'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m',\n",
       "       'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util',\n",
       "       'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct',\n",
       "       'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl',\n",
       "       'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n",
       "       'mths_since_recent_inq', 'mths_since_recent_revol_delinq',\n",
       "       'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl',\n",
       "       'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl',\n",
       "       'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats',\n",
       "       'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m',\n",
       "       'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75',\n",
       "       'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim',\n",
       "       'total_bal_ex_mort', 'total_bc_limit',\n",
       "       'total_il_high_credit_limit', 'revol_bal_joint',\n",
       "       'sec_app_fico_range_low', 'sec_app_fico_range_high',\n",
       "       'sec_app_earliest_cr_line', 'sec_app_inq_last_6mths',\n",
       "       'sec_app_mort_acc', 'sec_app_open_acc', 'sec_app_revol_util',\n",
       "       'sec_app_open_act_il', 'sec_app_num_rev_accts',\n",
       "       'sec_app_chargeoff_within_12_mths',\n",
       "       'sec_app_collections_12_mths_ex_med',\n",
       "       'sec_app_mths_since_last_major_derog', 'hardship_flag',\n",
       "       'hardship_type', 'hardship_reason', 'hardship_status',\n",
       "       'deferral_term', 'hardship_amount', 'hardship_start_date',\n",
       "       'hardship_end_date', 'payment_plan_start_date', 'hardship_length',\n",
       "       'hardship_dpd', 'hardship_loan_status',\n",
       "       'orig_projected_additional_accrued_interest',\n",
       "       'hardship_payoff_balance_amount', 'hardship_last_payment_amount',\n",
       "       'disbursement_method', 'debt_settlement_flag',\n",
       "       'debt_settlement_flag_date', 'settlement_status',\n",
       "       'settlement_date', 'settlement_amount', 'settlement_percentage',\n",
       "       'settlement_term'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(accepted_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Amount Requested', 'Application Date', 'Loan Title', 'Risk_Score',\n",
       "       'Debt-To-Income Ratio', 'Zip Code', 'State', 'Employment Length',\n",
       "       'Policy Code'], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(rejected_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(np.array(accepted_df.keys()), np.array(rejected_df.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Rejected Loans...\n",
      "Preprocessing Accepted Loans...\n",
      "Standardizing data types...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "CORE_FEATURES = [\n",
    "    'loan_amnt',\n",
    "    'dti',\n",
    "    'addr_state',\n",
    "    'emp_length',\n",
    "    'purpose',\n",
    "    'fico_score',       # Standardized FICO/Risk Score\n",
    "    'application_date', # Standardized date\n",
    "    'status'            # Target variable (1=Accepted, 0=Rejected)\n",
    "]\n",
    "\n",
    "# --- Preprocessing Functions ---\n",
    "\n",
    "def clean_emp_length(emp_length_str):\n",
    "    \"\"\"Cleans the employment length string into numeric years.\"\"\"\n",
    "    if pd.isna(emp_length_str) or emp_length_str in ['n/a', 'nan']:\n",
    "        return np.nan # Use NaN to represent missing/not applicable\n",
    "    elif emp_length_str == '< 1 year':\n",
    "        return 0\n",
    "    elif '10+ years' in emp_length_str:\n",
    "        return 10\n",
    "    else:\n",
    "        # Extract numbers using regex\n",
    "        match = re.search(r'\\d+', emp_length_str)\n",
    "        if match:\n",
    "            return int(match.group(0))\n",
    "        else:\n",
    "            return np.nan # Cannot parse\n",
    "\n",
    "\n",
    "# --- Step 1: Add Target Status ---\n",
    "accepted_df['status'] = 1\n",
    "rejected_df['status'] = 0\n",
    "\n",
    "# --- Step 2 & 3: Rename and Select Columns (Rejected) ---\n",
    "print(\"Preprocessing Rejected Loans...\")\n",
    "# Rename columns in rejected_df to match accepted_df conventions\n",
    "rename_map_rejected = {\n",
    "    'Amount Requested': 'loan_amnt',\n",
    "    'Application Date': 'application_date_str', # Keep temporary string name\n",
    "    'Loan Title': 'purpose',\n",
    "    'Risk_Score': 'fico_score',          # Assuming Risk_Score maps to FICO\n",
    "    'Debt-To-Income Ratio': 'dti_str',  # Keep temporary string name\n",
    "    'State': 'addr_state',\n",
    "    'Employment Length': 'emp_length_str', # Keep temporary string name\n",
    "    # Add 'Zip Code': 'zip_code' if needed\n",
    "    # Add 'Policy Code': 'policy_code' if needed\n",
    "}\n",
    "# Select only the columns we intend to map plus the status\n",
    "cols_to_keep_rejected = list(rename_map_rejected.keys()) + ['status']\n",
    "rejected_df_processed = rejected_df[cols_to_keep_rejected].copy()\n",
    "rejected_df_processed.rename(columns=rename_map_rejected, inplace=True)\n",
    "rejected_df_processed = rejected_df_processed.sort_index(axis=1)\n",
    "\n",
    "print(\"Preprocessing Accepted Loans...\")\n",
    "# Select corresponding columns plus the status\n",
    "# We need 'issue_d' for date, 'fico_range_low' for score, raw 'dti', raw 'emp_length'\n",
    "cols_to_keep_accepted = [\n",
    "    'loan_amnt',\n",
    "    'issue_d',          # Use as proxy for application date\n",
    "    'purpose',\n",
    "    'dti',              # Raw DTI\n",
    "    'addr_state',\n",
    "    'emp_length',       # Raw emp_length\n",
    "    'fico_range_low',   # Use low end of FICO range\n",
    "    'status'\n",
    "    # Add 'zip_code' if needed\n",
    "    # Add 'policy_code' if needed\n",
    "]\n",
    "# Filter out potential columns not present in older datasets if necessary\n",
    "available_cols_accepted = [col for col in cols_to_keep_accepted if col in accepted_df.columns]\n",
    "accepted_df_processed = accepted_df[available_cols_accepted].copy()\n",
    "\n",
    "# Rename accepted columns to standardized names\n",
    "accepted_df_processed.rename(columns={\n",
    "    'issue_d': 'application_date_str',\n",
    "    'fico_range_low': 'fico_score',\n",
    "    'emp_length': 'emp_length_str', # Keep temp name before cleaning\n",
    "    'dti': 'dti_str'             # Keep temp name before cleaning\n",
    "}, inplace=True)\n",
    "\n",
    "accepted_df_processed = accepted_df_processed.sort_index(axis=1)\n",
    "\n",
    "print(\"Standardizing data types...\")\n",
    "accepted_df_processed['application_date_str'] = pd.to_datetime(accepted_df_processed['application_date_str'], format='%b-%Y').dt.strftime('%Y-%m-%d')\n",
    "accepted_df_processed['application_date_str'] = pd.to_datetime(accepted_df_processed['application_date_str'], errors='coerce')\n",
    "rejected_df_processed['application_date_str'] = pd.to_datetime(rejected_df_processed['application_date_str'], errors='coerce')\n",
    "rejected_df_processed['dti_str'] = rejected_df_processed['dti_str'].str.replace('%', '').astype(float)\n",
    "emp_length_mapping = {\n",
    "    '< 1 year': 0,\n",
    "    '1 year': 1,\n",
    "    '2 years': 2,\n",
    "    '3 years': 3,\n",
    "    '4 years': 4,\n",
    "    '5 years': 5,\n",
    "    '6 years': 6,\n",
    "    '7 years': 7,\n",
    "    '8 years': 8,\n",
    "    '9 years': 9,\n",
    "    '10+ years': 10\n",
    "}\n",
    "\n",
    "accepted_df_processed['emp_length_str'] = accepted_df_processed['emp_length_str'].map(emp_length_mapping)\n",
    "rejected_df_processed['emp_length_str'] = rejected_df_processed['emp_length_str'].map(emp_length_mapping)\n",
    "accepted_df_processed['emp_length_str'] = accepted_df_processed['emp_length_str'].astype(float)\n",
    "rejected_df_processed['emp_length_str'] = rejected_df_processed['emp_length_str'].astype(float)\n",
    "accepted_df_processed['purpose'] = accepted_df_processed['purpose'].str.replace('_', ' ')\n",
    "rejected_df_processed['purpose'] = rejected_df_processed['purpose'].str.lower()\n",
    "\n",
    "accepted_final = accepted_df_processed.copy()\n",
    "rejected_final = rejected_df_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>addr_state</th>\n",
       "      <th>application_date_str</th>\n",
       "      <th>dti_str</th>\n",
       "      <th>emp_length_str</th>\n",
       "      <th>fico_score</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>purpose</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PA</td>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>5.91</td>\n",
       "      <td>10.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SD</td>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>16.06</td>\n",
       "      <td>10.0</td>\n",
       "      <td>715.0</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>small business</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IL</td>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>10.78</td>\n",
       "      <td>10.0</td>\n",
       "      <td>695.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>home improvement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NJ</td>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>17.06</td>\n",
       "      <td>10.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PA</td>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>25.37</td>\n",
       "      <td>3.0</td>\n",
       "      <td>695.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>major purchase</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  addr_state application_date_str  dti_str  emp_length_str  fico_score  \\\n",
       "0         PA           2015-12-01     5.91            10.0       675.0   \n",
       "1         SD           2015-12-01    16.06            10.0       715.0   \n",
       "2         IL           2015-12-01    10.78            10.0       695.0   \n",
       "3         NJ           2015-12-01    17.06            10.0       785.0   \n",
       "4         PA           2015-12-01    25.37             3.0       695.0   \n",
       "\n",
       "   loan_amnt             purpose  status  \n",
       "0     3600.0  debt consolidation       1  \n",
       "1    24700.0      small business       1  \n",
       "2    20000.0    home improvement       1  \n",
       "3    35000.0  debt consolidation       1  \n",
       "4    10400.0      major purchase       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>addr_state</th>\n",
       "      <th>application_date_str</th>\n",
       "      <th>dti_str</th>\n",
       "      <th>emp_length_str</th>\n",
       "      <th>fico_score</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>purpose</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NM</td>\n",
       "      <td>2007-05-26</td>\n",
       "      <td>10.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>wedding covered but no honeymoon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MA</td>\n",
       "      <td>2007-05-26</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>703.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>consolidating debt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MD</td>\n",
       "      <td>2007-05-27</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>715.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>want to consolidate my debt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MA</td>\n",
       "      <td>2007-05-27</td>\n",
       "      <td>38.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>698.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>waksman</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MD</td>\n",
       "      <td>2007-05-27</td>\n",
       "      <td>9.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>mdrigo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  addr_state application_date_str  dti_str  emp_length_str  fico_score  \\\n",
       "0         NM           2007-05-26    10.00             4.0       693.0   \n",
       "1         MA           2007-05-26    10.00             0.0       703.0   \n",
       "2         MD           2007-05-27    10.00             1.0       715.0   \n",
       "3         MA           2007-05-27    38.64             0.0       698.0   \n",
       "4         MD           2007-05-27     9.43             0.0       509.0   \n",
       "\n",
       "   loan_amnt                           purpose  status  \n",
       "0     1000.0  wedding covered but no honeymoon       0  \n",
       "1     1000.0                consolidating debt       0  \n",
       "2    11000.0       want to consolidate my debt       0  \n",
       "3     6000.0                           waksman       0  \n",
       "4     1500.0                            mdrigo       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "addr_state                      object\n",
       "application_date_str    datetime64[ns]\n",
       "dti_str                        float64\n",
       "emp_length_str                 float64\n",
       "fico_score                     float64\n",
       "loan_amnt                      float64\n",
       "purpose                         object\n",
       "status                           int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "addr_state                      object\n",
       "application_date_str    datetime64[ns]\n",
       "dti_str                        float64\n",
       "emp_length_str                 float64\n",
       "fico_score                     float64\n",
       "loan_amnt                      float64\n",
       "purpose                         object\n",
       "status                           int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(accepted_final.head())\n",
    "display(rejected_final.head())\n",
    "display(accepted_final.dtypes)\n",
    "display(rejected_final.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining datasets...\n",
      "Combined dataset shape: (29909442, 8)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Combine DataFrames ---\n",
    "print(\"Combining datasets...\")\n",
    "combined_df = pd.concat([accepted_final, rejected_final], ignore_index=True)\n",
    "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "\n",
    "# # Drop rows where the application date is missing, as it's crucial for temporal split\n",
    "# combined_df.dropna(subset=['application_date'], inplace=True)\n",
    "# print(f\"Shape after dropping rows with missing application date: {combined_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['application_date']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9b/6_8zt65542g2nv99zls1h_nr0000gn/T/ipykernel_25825/2187874080.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Drop rows where the application date is missing, as it's crucial for temporal split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcombined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'application_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape after dropping rows with missing application date: {combined_df.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6650\u001b[0m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6651\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6653\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6654\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6655\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ['application_date']"
     ]
    }
   ],
   "source": [
    "# Drop rows where the application date is missing, as it's crucial for temporal split\n",
    "combined_df.dropna(subset=['application_date'], inplace=True)\n",
    "print(f\"Shape after dropping rows with missing application date: {combined_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAVEN'T DONE MORE BELOW THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Rejected Loans...\n",
      "Preprocessing Accepted Loans...\n",
      "Standardizing data types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/6_8zt65542g2nv99zls1h_nr0000gn/T/ipykernel_6951/1503678181.py:115: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['application_date'] = pd.to_datetime(df['application_date_str'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted features selected: ['loan_amnt', 'dti', 'addr_state', 'emp_length', 'purpose', 'fico_score', 'application_date', 'status']\n",
      "Rejected features selected: ['loan_amnt', 'dti', 'addr_state', 'emp_length', 'purpose', 'fico_score', 'application_date', 'status']\n",
      "Combining datasets...\n",
      "Combined dataset shape: (29909442, 8)\n",
      "Shape after dropping rows with missing application date: (29909409, 8)\n",
      "Handling missing values...\n",
      "Missing values before imputation:\n",
      "loan_amnt                  0\n",
      "dti                     1711\n",
      "addr_state                 0\n",
      "emp_length           1098262\n",
      "purpose                    0\n",
      "fico_score          18497630\n",
      "application_date           0\n",
      "status                     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/6_8zt65542g2nv99zls1h_nr0000gn/T/ipykernel_6951/1503678181.py:176: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined_df[col].fillna(median_val, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed fico_score with median: 655.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/6_8zt65542g2nv99zls1h_nr0000gn/T/ipykernel_6951/1503678181.py:176: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined_df[col].fillna(median_val, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed dti with median: 19.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/6_8zt65542g2nv99zls1h_nr0000gn/T/ipykernel_6951/1503678181.py:176: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined_df[col].fillna(median_val, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed emp_length with median: 0.0\n",
      "Imputed loan_amnt with mean: 13277.88\n",
      "\n",
      "Missing values after imputation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/6_8zt65542g2nv99zls1h_nr0000gn/T/ipykernel_6951/1503678181.py:182: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined_df[col].fillna(mean_val, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amnt           0\n",
      "dti                 0\n",
      "addr_state          0\n",
      "emp_length          0\n",
      "purpose             0\n",
      "fico_score          0\n",
      "application_date    0\n",
      "status              0\n",
      "dtype: int64\n",
      "Encoding categorical features...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Step 6: Combine DataFrames ---\n",
    "print(\"Combining datasets...\")\n",
    "combined_df = pd.concat([accepted_final, rejected_final], ignore_index=True)\n",
    "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "\n",
    "# Drop rows where the application date is missing, as it's crucial for temporal split\n",
    "combined_df.dropna(subset=['application_date'], inplace=True)\n",
    "print(f\"Shape after dropping rows with missing application date: {combined_df.shape}\")\n",
    "\n",
    "\n",
    "# --- Step 7: Handle Missing Values ---\n",
    "print(\"Handling missing values...\")\n",
    "print(\"Missing values before imputation:\")\n",
    "print(combined_df.isnull().sum())\n",
    "\n",
    "# Imputation Strategies (Example - adjust as needed):\n",
    "# - Median for numerical skewed data (fico, dti, potentially emp_length if treated num)\n",
    "# - Mean for less skewed numerical (loan_amnt?)\n",
    "# - Mode or 'Missing' category for categorical (purpose, addr_state)\n",
    "# - For emp_length, NaN might mean 'not applicable' or truly missing. Median or 0 could work.\n",
    "\n",
    "impute_median_cols = ['fico_score', 'dti', 'emp_length'] # Example\n",
    "impute_mean_cols = ['loan_amnt'] # Example\n",
    "\n",
    "for col in impute_median_cols:\n",
    "    if col in combined_df.columns:\n",
    "        median_val = combined_df[col].median()\n",
    "        combined_df[col].fillna(median_val, inplace=True)\n",
    "        print(f\"Imputed {col} with median: {median_val}\")\n",
    "\n",
    "for col in impute_mean_cols:\n",
    "     if col in combined_df.columns:\n",
    "        mean_val = combined_df[col].mean()\n",
    "        combined_df[col].fillna(mean_val, inplace=True)\n",
    "        print(f\"Imputed {col} with mean: {mean_val:.2f}\")\n",
    "\n",
    "# For categorical, imputation with mode or a new category 'Missing'\n",
    "# For simplicity here, we'll rely on dropping rows if critical info like FICO was missing earlier\n",
    "# Or ensure imputation covers the key columns. Check again:\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(combined_df.isnull().sum())\n",
    "# Consider dropping rows if key features still have NaNs after imputation if appropriate\n",
    "# combined_df.dropna(subset=['fico_score', 'dti', 'emp_length'], inplace=True)\n",
    "\n",
    "\n",
    "# --- Step 8: Encode Categorical Features ---\n",
    "print(\"Encoding categorical features...\")\n",
    "# Identify categorical columns (object type or explicitly known)\n",
    "categorical_cols = ['purpose', 'addr_state'] # Add others if kept (e.g., policy_code)\n",
    "\n",
    "# Use One-Hot Encoding\n",
    "combined_df_encoded = pd.get_dummies(combined_df, columns=categorical_cols, drop_first=True) # drop_first helps reduce multicollinearity\n",
    "print(f\"Shape after one-hot encoding: {combined_df_encoded.shape}\")\n",
    "print(\"Example encoded columns:\", combined_df_encoded.columns[:10].tolist(), \"...\") # Show some new columns\n",
    "\n",
    "\n",
    "# --- Step 9: Extract Year and Temporal Split ---\n",
    "print(\"Splitting data by time period...\")\n",
    "combined_df_encoded['year'] = combined_df_encoded['application_date'].dt.year\n",
    "\n",
    "# Define time periods\n",
    "recession_years = range(2007, 2013) # 2007-2012 inclusive\n",
    "post_recession_years = range(2013, 2019) # 2013-2018 inclusive\n",
    "\n",
    "recession_df = combined_df_encoded[combined_df_encoded['year'].isin(recession_years)].copy()\n",
    "post_recession_df = combined_df_encoded[combined_df_encoded['year'].isin(post_recession_years)].copy()\n",
    "\n",
    "# Drop the date and year columns as they are no longer needed for modeling itself\n",
    "recession_df.drop(columns=['application_date', 'year'], inplace=True)\n",
    "post_recession_df.drop(columns=['application_date', 'year'], inplace=True)\n",
    "\n",
    "print(f\"Recession Era (2007-2012) dataset shape: {recession_df.shape}\")\n",
    "print(f\"Post-Recession Era (2013-2018) dataset shape: {post_recession_df.shape}\")\n",
    "\n",
    "# --- Final Check ---\n",
    "print(\"\\nPreprocessing Complete.\")\n",
    "print(\"\\nRecession Data Info:\")\n",
    "recession_df.info()\n",
    "print(\"\\nPost-Recession Data Info:\")\n",
    "post_recession_df.info()\n",
    "\n",
    "# --- Ready for Modeling ---\n",
    "# You can now save these processed dataframes or use them directly\n",
    "# Example:\n",
    "# recession_df.to_csv('recession_processed.csv', index=False)\n",
    "# post_recession_df.to_csv('post_recession_processed.csv', index=False)\n",
    "\n",
    "# Now you have recession_df and post_recession_df ready for your ML models.\n",
    "# Remember to separate features (X) and target (y = 'status') before training.\n",
    "# Example for recession data:\n",
    "# X_recession = recession_df.drop('status', axis=1)\n",
    "# y_recession = recession_df['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to partition datasets by time period\n",
    "def partition_by_time(accepted_df, rejected_df):\n",
    "    # For accepted loans\n",
    "    # Convert issue_d to datetime format\n",
    "    accepted_df['issue_d'] = pd.to_datetime(accepted_df['issue_d'])\n",
    "\n",
    "    # Create the time-based partitions\n",
    "    recession_accepted = accepted_df[(accepted_df['issue_d'] >= '2007-01-01') & \n",
    "                                    (accepted_df['issue_d'] <= '2012-12-31')]\n",
    "    post_recession_accepted = accepted_df[(accepted_df['issue_d'] >= '2013-01-01') & \n",
    "                                        (accepted_df['issue_d'] <= '2018-12-31')]\n",
    "\n",
    "    # For rejected loans\n",
    "    # Convert Application Date to datetime\n",
    "    rejected_df['Application Date'] = pd.to_datetime(rejected_df['Application Date'])\n",
    "\n",
    "    # Create the time-based partitions\n",
    "    recession_rejected = rejected_df[(rejected_df['Application Date'] >= '2007-01-01') & \n",
    "                                    (rejected_df['Application Date'] <= '2012-12-31')]\n",
    "    post_recession_rejected = rejected_df[(rejected_df['Application Date'] >= '2013-01-01') & \n",
    "                                        (rejected_df['Application Date'] <= '2018-12-31')]\n",
    "    \n",
    "    return recession_accepted, post_recession_accepted, recession_rejected, post_recession_rejected\n",
    "\n",
    "def prepare_features(accepted_df, rejected_df):\n",
    "    # Common features between datasets (with mapping)\n",
    "    feature_mapping = {\n",
    "        'accepted': {\n",
    "            'loan_amnt': 'Amount Requested',\n",
    "            'zip_code': 'Zip Code',\n",
    "            'fico_range_low': 'Risk Score',\n",
    "            'addr_state': 'State',\n",
    "            'emp_length': 'Employment Length',\n",
    "            'dti': 'Debt-To-Income Ratio',\n",
    "            'title': 'Loan Title'\n",
    "        },\n",
    "        'rejected': {\n",
    "            'Amount Requested': 'Amount Requested',\n",
    "            'Zip Code': 'Zip Code',\n",
    "            'Risk_Score': 'Risk Score',\n",
    "            'State': 'State',\n",
    "            'Employment Length': 'Employment Length',\n",
    "            'Debt-To-Income Ratio': 'Debt-To-Income Ratio',\n",
    "            'Loan Title': 'Loan Title'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Add target column (1 for accepted, 0 for rejected)\n",
    "    accepted_features = recession_accepted[list(feature_mapping['accepted'].keys())].copy()\n",
    "    accepted_features['status'] = 1\n",
    "\n",
    "    rejected_features = recession_rejected[list(feature_mapping['rejected'].keys())].copy()\n",
    "    rejected_features['status'] = 0\n",
    "\n",
    "    accepted_features = accepted_features.rename(columns=feature_mapping['accepted'])\n",
    "    rejected_features = rejected_features.rename(columns=feature_mapping['rejected'])\n",
    "\n",
    "    def convert_employment_length(s):\n",
    "        if pd.isna(s):\n",
    "            return np.nan\n",
    "        s = str(s).lower()\n",
    "        if '<' in s:\n",
    "            return 0.5  # Treat \"< 1 year\" as 0 years\n",
    "        if '+' in s:\n",
    "            match = re.search(r'(\\d+)\\+', s)  # Extract number before \"+\"\n",
    "            if match:\n",
    "                return float(match.group(1)) + 2\n",
    "        # Extract the first number found\n",
    "        match = re.search(r'\\d+', s)\n",
    "        if match:\n",
    "            return float(match.group())\n",
    "        else:\n",
    "            return np.nan  # Handle cases with no numbers\n",
    "\n",
    "    # Apply the function to the column\n",
    "    rejected_features['Employment Length'] = rejected_features['Employment Length'].apply(convert_employment_length)\n",
    "    accepted_features['Employment Length'] = accepted_features['Employment Length'].apply(convert_employment_length)\n",
    "\n",
    "    # Remove \"%\" and convert to float\n",
    "    rejected_features['Debt-To-Income Ratio'] = (\n",
    "        rejected_features['Debt-To-Income Ratio']\n",
    "        .str.replace('%', '')\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([accepted_features, rejected_features], ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to train and evaluate multiple models\n",
    "def build_models(X_train, X_test, y_train, y_test):\n",
    "    # Initialize models with parameter grids for grid search\n",
    "    models = {\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "            }\n",
    "        },\n",
    "        'KNN': {\n",
    "            'model': KNeighborsClassifier(),\n",
    "            'params': {\n",
    "                'n_neighbors': [3, 5, 7, 10],\n",
    "                'weights': ['uniform', 'distance']\n",
    "            }\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'model': xgb.XGBClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.01, 0.1, 0.2]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    best_models = {}\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for model_name, model_info in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model_info['model'])\n",
    "        ])\n",
    "        \n",
    "        param_grid = {'model__' + key: value for key, value in model_info['params'].items()}\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "        results[model_name] = evaluate_model(grid_search.best_estimator_, X_test, y_test)\n",
    "        results[model_name]['best_params'] = grid_search.best_params_\n",
    "        \n",
    "        print(f\"{model_name} trained. Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"ROC AUC: {results[model_name]['roc_auc']:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return results, best_models\n",
    "\n",
    "# Function to visualize results\n",
    "def visualize_results(results, era_name):\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Bar chart for metrics\n",
    "    plt.subplot(2, 2, 1)\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    model_names = list(results.keys())\n",
    "    \n",
    "    # Create a dataframe for easy plotting\n",
    "    metrics_df = pd.DataFrame({\n",
    "        model_name: [results[model_name][metric] for metric in metrics]\n",
    "        for model_name in model_names\n",
    "    }, index=metrics)\n",
    "    \n",
    "    metrics_df.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title(f'Model Performance Metrics - {era_name}')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        plt.subplot(2, 2, i+2)\n",
    "        cm = results[model_name]['confusion_matrix']\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                  xticklabels=['Rejected', 'Accepted'],\n",
    "                  yticklabels=['Rejected', 'Accepted'])\n",
    "        plt.title(f'Confusion Matrix - {model_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'loan_model_comparison_{era_name}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return f'loan_model_comparison_{era_name}.png'\n",
    "\n",
    "# Function to visualize feature importance\n",
    "def visualize_feature_importance(model, feature_names, era_name, model_name):\n",
    "    # Extract feature importances if available\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        importances = np.abs(model.coef_[0])\n",
    "    else:\n",
    "        print(f\"Model {model_name} doesn't provide feature importances\")\n",
    "        return None\n",
    "    \n",
    "    # Create a DataFrame for visualization\n",
    "    importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=importance_df)\n",
    "    plt.title(f'Feature Importance - {era_name} - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'feature_importance_{era_name}_{model_name}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return importance_df.head(10)\n",
    "\n",
    "# Main function to analyze lending patterns\n",
    "def analyze_lending_patterns(accepted_df, rejected_df):\n",
    "    # Partition datasets by time period\n",
    "    partitioned_data = partition_by_time(accepted_df, rejected_df)\n",
    "    \n",
    "    era_results = {}\n",
    "    era_models = {}\n",
    "    era_feature_importance = {}\n",
    "    \n",
    "    # For each era, prepare data, build and evaluate models\n",
    "    for era_name, era_data in partitioned_data.items():\n",
    "        print(f\"\\n{'='*50}\\nAnalyzing {era_name} era\\n{'='*50}\")\n",
    "        print(f\"Accepted loans: {era_data['accepted'].shape[0]}\")\n",
    "        print(f\"Rejected loans: {era_data['rejected'].shape[0]}\")\n",
    "        \n",
    "        # Prepare features for the current era\n",
    "        combined_df = prepare_features(era_data['accepted'], era_data['rejected'])\n",
    "        \n",
    "        # Print information about the dataset\n",
    "        print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "        print(f\"Acceptance rate: {combined_df['status'].mean():.2%}\")\n",
    "        \n",
    "        # Handle potential class imbalance with sampling if needed\n",
    "        accepted_count = combined_df[combined_df['status'] == 1].shape[0]\n",
    "        rejected_count = combined_df[combined_df['status'] == 0].shape[0]\n",
    "        \n",
    "        # If severe imbalance, consider downsampling the majority class\n",
    "        if rejected_count > 3 * accepted_count:\n",
    "            print(f\"Downsampling rejected loans from {rejected_count} to {3 * accepted_count}\")\n",
    "            rejected_sample = combined_df[combined_df['status'] == 0].sample(\n",
    "                n=min(rejected_count, 3 * accepted_count), \n",
    "                random_state=42\n",
    "            )\n",
    "            accepted_sample = combined_df[combined_df['status'] == 1]\n",
    "            combined_df = pd.concat([accepted_sample, rejected_sample])\n",
    "            print(f\"New dataset shape after balancing: {combined_df.shape}\")\n",
    "            print(f\"New acceptance rate: {combined_df['status'].mean():.2%}\")\n",
    "        \n",
    "        # Split features and target\n",
    "        X = combined_df.drop('status', axis=1)\n",
    "        y = combined_df['status']\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "        print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "        \n",
    "        # Build and evaluate models\n",
    "        results, models = build_models(X_train, X_test, y_train, y_test)\n",
    "        era_results[era_name] = results\n",
    "        era_models[era_name] = models\n",
    "        era_feature_importance[era_name] = {}\n",
    "        \n",
    "        # Extract and visualize feature importance for tree-based models\n",
    "        for model_name, model in models.items():\n",
    "            if model_name in ['RandomForest', 'XGBoost']:\n",
    "                # Extract the actual model from the pipeline\n",
    "                estimator = model.named_steps['model']\n",
    "                importance_df = visualize_feature_importance(estimator, X.columns, era_name, model_name)\n",
    "                if importance_df is not None:\n",
    "                    era_feature_importance[era_name][model_name] = importance_df\n",
    "        \n",
    "        # Visualize results\n",
    "        viz_filename = visualize_results(results, era_name)\n",
    "        print(f\"Results visualization saved to {viz_filename}\")\n",
    "    \n",
    "    # Compare best models across eras\n",
    "    best_model_comparison = {}\n",
    "    for era_name, results in era_results.items():\n",
    "        # Find the best model for this era based on ROC AUC\n",
    "        best_model_name = max(results.items(), key=lambda x: x[1]['roc_auc'])[0]\n",
    "        best_model_metrics = results[best_model_name]\n",
    "        \n",
    "        best_model_comparison[era_name] = {\n",
    "            'best_model': best_model_name,\n",
    "            'metrics': {\n",
    "                metric: value for metric, value in best_model_metrics.items() \n",
    "                if metric != 'confusion_matrix' and metric != 'best_params'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Print comparative summary\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"COMPARATIVE SUMMARY ACROSS ERAS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for era_name, info in best_model_comparison.items():\n",
    "        print(f\"\\n{era_name.upper()} ERA:\")\n",
    "        print(f\"Best model: {info['best_model']}\")\n",
    "        print(\"Performance metrics:\")\n",
    "        for metric, value in info['metrics'].items():\n",
    "            if metric != 'confusion_matrix' and metric != 'best_params':\n",
    "                print(f\"  - {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Compare feature importance across eras\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"FEATURE IMPORTANCE COMPARISON ACROSS ERAS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Find common model type to compare\n",
    "    common_model_types = set.intersection(\n",
    "        *[set(era_fi.keys()) for era_fi in era_feature_importance.values()]\n",
    "    )\n",
    "    \n",
    "    if common_model_types:\n",
    "        model_type = list(common_model_types)[0]\n",
    "        print(f\"\\nComparing feature importance for {model_type} across eras:\")\n",
    "        \n",
    "        # Create a DataFrame to compare top features\n",
    "        all_features = set()\n",
    "        for era_name in era_feature_importance:\n",
    "            if model_type in era_feature_importance[era_name]:\n",
    "                all_features.update(era_feature_importance[era_name][model_type]['feature'])\n",
    "        \n",
    "        comparison_df = pd.DataFrame(0, index=sorted(all_features), columns=era_results.keys())\n",
    "        \n",
    "        for era_name in era_feature_importance:\n",
    "            if model_type in era_feature_importance[era_name]:\n",
    "                for _, row in era_feature_importance[era_name][model_type].iterrows():\n",
    "                    feature = row['feature']\n",
    "                    importance = row['importance']\n",
    "                    comparison_df.at[feature, era_name] = importance\n",
    "        \n",
    "        # Sort by average importance\n",
    "        comparison_df['avg'] = comparison_df.mean(axis=1)\n",
    "        comparison_df = comparison_df.sort_values('avg', ascending=False).drop('avg', axis=1)\n",
    "        \n",
    "        print(\"\\nTop 10 features by importance across eras:\")\n",
    "        print(comparison_df.head(10))\n",
    "        \n",
    "        # Visualize comparison\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        comparison_df.head(10).plot(kind='bar')\n",
    "        plt.title(f'Feature Importance Comparison Across Eras - {model_type}')\n",
    "        plt.ylabel('Importance Score')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'feature_importance_comparison_{model_type}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\nFeature importance comparison visualization saved to feature_importance_comparison_{model_type}.png\")\n",
    "    \n",
    "    return era_results, era_models, best_model_comparison, era_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "recession_accepted, post_recession_accepted, recession_rejected, post_recession_rejected = partition_by_time(accepted_df, rejected_df)\n",
    "\n",
    "partitioned_data = {\n",
    "    \"recession\": [recession_accepted, recession_rejected],\n",
    "    \"post-recession\": [post_recession_accepted, post_recession_rejected]\n",
    "}\n",
    "era_results = {}\n",
    "era_models = {}\n",
    "era_feature_importance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (851393, 8)\n",
      "Acceptance rate: 11.26%\n",
      "Downsampling rejected loans from 755491 to 287706\n",
      "New dataset shape after balancing: (383608, 8)\n",
      "New acceptance rate: 25.00%\n"
     ]
    }
   ],
   "source": [
    "combined_df = prepare_features(recession_accepted, recession_rejected)\n",
    "combined_df.head()\n",
    "\n",
    "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "print(f\"Acceptance rate: {combined_df['status'].mean():.2%}\")\n",
    "\n",
    "# Handle potential class imbalance with sampling if needed\n",
    "accepted_count = combined_df[combined_df['status'] == 1].shape[0]\n",
    "rejected_count = combined_df[combined_df['status'] == 0].shape[0]\n",
    "\n",
    "# If severe imbalance, consider downsampling the majority class\n",
    "if rejected_count > 3 * accepted_count:\n",
    "    print(f\"Downsampling rejected loans from {rejected_count} to {3 * accepted_count}\")\n",
    "    rejected_sample = combined_df[combined_df['status'] == 0].sample(\n",
    "        n=min(rejected_count, 3 * accepted_count), \n",
    "        random_state=42\n",
    "    )\n",
    "    accepted_sample = combined_df[combined_df['status'] == 1]\n",
    "    combined_df = pd.concat([accepted_sample, rejected_sample])\n",
    "    print(f\"New dataset shape after balancing: {combined_df.shape}\")\n",
    "    print(f\"New acceptance rate: {combined_df['status'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 306886 samples\n",
      "Test set size: 76722 samples\n"
     ]
    }
   ],
   "source": [
    "X = combined_df.drop('status', axis=1)\n",
    "y = combined_df['status']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForest...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 54 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n18 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 894, in fit\n    return self.partial_fit(X, y, sample_weight)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 930, in partial_fit\n    X = validate_data(\n        ^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 2944, in validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1055, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 839, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/pandas/core/generic.py\", line 2150, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: '902xx'\n\n--------------------------------------------------------------------------------\n36 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 894, in fit\n    return self.partial_fit(X, y, sample_weight)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 930, in partial_fit\n    X = validate_data(\n        ^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 2944, in validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1055, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 839, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/pandas/core/generic.py\", line 2150, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: '452xx'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Build and evaluate models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results, models \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m era_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecession\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m results\n\u001b[1;32m      4\u001b[0m era_models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecession\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m models\n",
      "Cell \u001b[0;32mIn[47], line 162\u001b[0m, in \u001b[0;36mbuild_models\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m    160\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m model_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    161\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m best_models[model_name] \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m    165\u001b[0m results[model_name] \u001b[38;5;241m=\u001b[39m evaluate_model(grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_, X_test, y_test)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1001\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    999\u001b[0m     )\n\u001b[0;32m-> 1001\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m     )\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 54 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n18 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 894, in fit\n    return self.partial_fit(X, y, sample_weight)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 930, in partial_fit\n    X = validate_data(\n        ^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 2944, in validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1055, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 839, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/pandas/core/generic.py\", line 2150, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: '902xx'\n\n--------------------------------------------------------------------------------\n36 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 894, in fit\n    return self.partial_fit(X, y, sample_weight)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\", line 930, in partial_fit\n    X = validate_data(\n        ^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 2944, in validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1055, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 839, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/pandas/core/generic.py\", line 2150, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: '452xx'\n"
     ]
    }
   ],
   "source": [
    "# Build and evaluate models\n",
    "results, models = build_models(X_train, X_test, y_train, y_test)\n",
    "era_results[\"recession\"] = results\n",
    "era_models[\"recession\"] = models\n",
    "era_feature_importance[\"recession\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition datasets by time period\n",
    "partitioned_data = partition_by_time(accepted_df, rejected_df)\n",
    "\n",
    "era_results = {}\n",
    "era_models = {}\n",
    "era_feature_importance = {}\n",
    "\n",
    "# For each era, prepare data, build and evaluate models\n",
    "for era_name, era_data in partitioned_data.items():\n",
    "    print(f\"\\n{'='*50}\\nAnalyzing {era_name} era\\n{'='*50}\")\n",
    "    print(f\"Accepted loans: {era_data['accepted'].shape[0]}\")\n",
    "    print(f\"Rejected loans: {era_data['rejected'].shape[0]}\")\n",
    "    \n",
    "    # Prepare features for the current era\n",
    "    combined_df = prepare_features(era_data['accepted'], era_data['rejected'])\n",
    "    \n",
    "    # Print information about the dataset\n",
    "    print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "    print(f\"Acceptance rate: {combined_df['status'].mean():.2%}\")\n",
    "    \n",
    "    # Handle potential class imbalance with sampling if needed\n",
    "    accepted_count = combined_df[combined_df['status'] == 1].shape[0]\n",
    "    rejected_count = combined_df[combined_df['status'] == 0].shape[0]\n",
    "    \n",
    "    # If severe imbalance, consider downsampling the majority class\n",
    "    if rejected_count > 3 * accepted_count:\n",
    "        print(f\"Downsampling rejected loans from {rejected_count} to {3 * accepted_count}\")\n",
    "        rejected_sample = combined_df[combined_df['status'] == 0].sample(\n",
    "            n=min(rejected_count, 3 * accepted_count), \n",
    "            random_state=42\n",
    "        )\n",
    "        accepted_sample = combined_df[combined_df['status'] == 1]\n",
    "        combined_df = pd.concat([accepted_sample, rejected_sample])\n",
    "        print(f\"New dataset shape after balancing: {combined_df.shape}\")\n",
    "        print(f\"New acceptance rate: {combined_df['status'].mean():.2%}\")\n",
    "    \n",
    "    # Split features and target\n",
    "    X = combined_df.drop('status', axis=1)\n",
    "    y = combined_df['status']\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Build and evaluate models\n",
    "    results, models = build_models(X_train, X_test, y_train, y_test)\n",
    "    era_results[era_name] = results\n",
    "    era_models[era_name] = models\n",
    "    era_feature_importance[era_name] = {}\n",
    "    \n",
    "    # Extract and visualize feature importance for tree-based models\n",
    "    for model_name, model in models.items():\n",
    "        if model_name in ['RandomForest', 'XGBoost']:\n",
    "            # Extract the actual model from the pipeline\n",
    "            estimator = model.named_steps['model']\n",
    "            importance_df = visualize_feature_importance(estimator, X.columns, era_name, model_name)\n",
    "            if importance_df is not None:\n",
    "                era_feature_importance[era_name][model_name] = importance_df\n",
    "    \n",
    "    # Visualize results\n",
    "    viz_filename = visualize_results(results, era_name)\n",
    "    print(f\"Results visualization saved to {viz_filename}\")\n",
    "\n",
    "# Compare best models across eras\n",
    "best_model_comparison = {}\n",
    "for era_name, results in era_results.items():\n",
    "    # Find the best model for this era based on ROC AUC\n",
    "    best_model_name = max(results.items(), key=lambda x: x[1]['roc_auc'])[0]\n",
    "    best_model_metrics = results[best_model_name]\n",
    "    \n",
    "    best_model_comparison[era_name] = {\n",
    "        'best_model': best_model_name,\n",
    "        'metrics': {\n",
    "            metric: value for metric, value in best_model_metrics.items() \n",
    "            if metric != 'confusion_matrix' and metric != 'best_params'\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Print comparative summary\n",
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"COMPARATIVE SUMMARY ACROSS ERAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for era_name, info in best_model_comparison.items():\n",
    "    print(f\"\\n{era_name.upper()} ERA:\")\n",
    "    print(f\"Best model: {info['best_model']}\")\n",
    "    print(\"Performance metrics:\")\n",
    "    for metric, value in info['metrics'].items():\n",
    "        if metric != 'confusion_matrix' and metric != 'best_params':\n",
    "            print(f\"  - {metric}: {value:.4f}\")\n",
    "\n",
    "# Compare feature importance across eras\n",
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"FEATURE IMPORTANCE COMPARISON ACROSS ERAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find common model type to compare\n",
    "common_model_types = set.intersection(\n",
    "    *[set(era_fi.keys()) for era_fi in era_feature_importance.values()]\n",
    ")\n",
    "\n",
    "if common_model_types:\n",
    "    model_type = list(common_model_types)[0]\n",
    "    print(f\"\\nComparing feature importance for {model_type} across eras:\")\n",
    "    \n",
    "    # Create a DataFrame to compare top features\n",
    "    all_features = set()\n",
    "    for era_name in era_feature_importance:\n",
    "        if model_type in era_feature_importance[era_name]:\n",
    "            all_features.update(era_feature_importance[era_name][model_type]['feature'])\n",
    "    \n",
    "    comparison_df = pd.DataFrame(0, index=sorted(all_features), columns=era_results.keys())\n",
    "    \n",
    "    for era_name in era_feature_importance:\n",
    "        if model_type in era_feature_importance[era_name]:\n",
    "            for _, row in era_feature_importance[era_name][model_type].iterrows():\n",
    "                feature = row['feature']\n",
    "                importance = row['importance']\n",
    "                comparison_df.at[feature, era_name] = importance\n",
    "    \n",
    "    # Sort by average importance\n",
    "    comparison_df['avg'] = comparison_df.mean(axis=1)\n",
    "    comparison_df = comparison_df.sort_values('avg', ascending=False).drop('avg', axis=1)\n",
    "    \n",
    "    print(\"\\nTop 10 features by importance across eras:\")\n",
    "    print(comparison_df.head(10))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    comparison_df.head(10).plot(kind='bar')\n",
    "    plt.title(f'Feature Importance Comparison Across Eras - {model_type}')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'feature_importance_comparison_{model_type}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nFeature importance comparison visualization saved to feature_importance_comparison_{model_type}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Analyzing recession era\n",
      "==================================================\n",
      "Accepted loans: 95902\n",
      "Rejected loans: 755491\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results, models, comparison, feature_importance = analyze_lending_patterns(accepted_df, rejected_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
