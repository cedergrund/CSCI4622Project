{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Read ReadMe for instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgboost\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/6_8zt65542g2nv99zls1h_nr0000gn/T/ipykernel_45015/3125798192.py:1: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  accepted_df = pd.read_csv(\"datasets/accepted_dataset.csv\")\n"
     ]
    }
   ],
   "source": [
    "accepted_df = pd.read_csv(\"datasets/accepted_dataset.csv\")\n",
    "rejected_df = pd.read_csv(\"datasets/rejected_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Rejected Loans...\n",
      "Preprocessing Accepted Loans...\n",
      "Standardizing data types...\n",
      "Handling missing values...\n",
      "Missing values before imputation:\n",
      "addr_state               0\n",
      "application_date         0\n",
      "dti                   1711\n",
      "emp_length          146907\n",
      "fico_score               0\n",
      "loan_amnt                0\n",
      "purpose                  0\n",
      "status                   0\n",
      "dtype: int64\n",
      "addr_state                22\n",
      "application_date           0\n",
      "dti                        0\n",
      "emp_length            951355\n",
      "fico_score          18497630\n",
      "loan_amnt                  0\n",
      "purpose                 1305\n",
      "status                     0\n",
      "dtype: int64\n",
      "Dropping null\n",
      "Splitting data by time period...\n",
      "\n",
      "Recession Era (2007-2012) dataset shape:\n",
      " -> Accepted: 92865\n",
      " -> Rejected: 723523\n",
      "\n",
      "Post-Recession Era (2013-2018) dataset shape:\n",
      " -> Accepted: 2020783\n",
      " -> Rejected: 8269041\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Add Target Status ---\n",
    "accepted_df['status'] = 1\n",
    "rejected_df['status'] = 0\n",
    "\n",
    "# --- Step 2 & 3: Rename and Select Columns (Rejected) ---\n",
    "print(\"Preprocessing Rejected Loans...\")\n",
    "# Rename columns in rejected_df to match accepted_df conventions\n",
    "rename_map_rejected = {\n",
    "    'Amount Requested': 'loan_amnt',\n",
    "    'Application Date': 'application_date_str', # Keep temporary string name\n",
    "    'Loan Title': 'purpose',\n",
    "    'Risk_Score': 'fico_score',          # Assuming Risk_Score maps to FICO\n",
    "    'Debt-To-Income Ratio': 'dti_str',  # Keep temporary string name\n",
    "    'State': 'addr_state',\n",
    "    'Employment Length': 'emp_length_str', # Keep temporary string name\n",
    "    # Add 'Zip Code': 'zip_code' if needed\n",
    "    # Add 'Policy Code': 'policy_code' if needed\n",
    "}\n",
    "# Select only the columns we intend to map plus the status\n",
    "cols_to_keep_rejected = list(rename_map_rejected.keys()) + ['status']\n",
    "rejected_df_processed = rejected_df[cols_to_keep_rejected].copy()\n",
    "rejected_df_processed.rename(columns=rename_map_rejected, inplace=True)\n",
    "rejected_df_processed = rejected_df_processed.sort_index(axis=1)\n",
    "\n",
    "print(\"Preprocessing Accepted Loans...\")\n",
    "# Select corresponding columns plus the status\n",
    "# We need 'issue_d' for date, 'fico_range_low' for score, raw 'dti', raw 'emp_length'\n",
    "cols_to_keep_accepted = [\n",
    "    'loan_amnt',\n",
    "    'issue_d',          # Use as proxy for application date\n",
    "    'purpose',\n",
    "    'dti',              # Raw DTI\n",
    "    'addr_state',\n",
    "    'emp_length',       # Raw emp_length\n",
    "    'fico_range_low',   # Use low end of FICO range\n",
    "    'status'\n",
    "    # Add 'zip_code' if needed\n",
    "    # Add 'policy_code' if needed\n",
    "]\n",
    "# Filter out potential columns not present in older datasets if necessary\n",
    "available_cols_accepted = [col for col in cols_to_keep_accepted if col in accepted_df.columns]\n",
    "accepted_df_processed = accepted_df[available_cols_accepted].copy()\n",
    "\n",
    "# Rename accepted columns to standardized names\n",
    "accepted_df_processed.rename(columns={\n",
    "    'issue_d': 'application_date_str',\n",
    "    'fico_range_low': 'fico_score',\n",
    "    'emp_length': 'emp_length_str', # Keep temp name before cleaning\n",
    "    'dti': 'dti_str'             # Keep temp name before cleaning\n",
    "}, inplace=True)\n",
    "\n",
    "accepted_df_processed = accepted_df_processed.sort_index(axis=1)\n",
    "\n",
    "print(\"Standardizing data types...\")\n",
    "accepted_df_processed['application_date_str'] = pd.to_datetime(accepted_df_processed['application_date_str'], format='%b-%Y').dt.strftime('%Y-%m-%d')\n",
    "accepted_df_processed['application_date_str'] = pd.to_datetime(accepted_df_processed['application_date_str'], errors='coerce')\n",
    "rejected_df_processed['application_date_str'] = pd.to_datetime(rejected_df_processed['application_date_str'], errors='coerce')\n",
    "rejected_df_processed['dti_str'] = rejected_df_processed['dti_str'].str.replace('%', '').astype(float)\n",
    "emp_length_mapping = {\n",
    "    '< 1 year': 0,\n",
    "    '1 year': 1,\n",
    "    '2 years': 2,\n",
    "    '3 years': 3,\n",
    "    '4 years': 4,\n",
    "    '5 years': 5,\n",
    "    '6 years': 6,\n",
    "    '7 years': 7,\n",
    "    '8 years': 8,\n",
    "    '9 years': 9,\n",
    "    '10+ years': 10\n",
    "}\n",
    "\n",
    "accepted_df_processed['emp_length_str'] = accepted_df_processed['emp_length_str'].map(emp_length_mapping)\n",
    "rejected_df_processed['emp_length_str'] = rejected_df_processed['emp_length_str'].map(emp_length_mapping)\n",
    "accepted_df_processed['emp_length_str'] = accepted_df_processed['emp_length_str'].astype(float)\n",
    "rejected_df_processed['emp_length_str'] = rejected_df_processed['emp_length_str'].astype(float)\n",
    "accepted_df_processed['purpose'] = accepted_df_processed['purpose'].str.replace('_', ' ')\n",
    "rejected_df_processed['purpose'] = rejected_df_processed['purpose'].str.replace('_', ' ')\n",
    "accepted_df_processed['purpose'] = accepted_df_processed['purpose'].str.lower()\n",
    "rejected_df_processed['purpose'] = rejected_df_processed['purpose'].str.lower()\n",
    "\n",
    "accepted_final = accepted_df_processed.copy()\n",
    "rejected_final = rejected_df_processed.copy()\n",
    "\n",
    "accepted_final = accepted_final.rename(columns={'application_date_str': 'application_date', 'emp_length_str':'emp_length', 'dti_str':'dti'})\n",
    "rejected_final = rejected_final.rename(columns={'application_date_str': 'application_date', 'emp_length_str':'emp_length', 'dti_str':'dti'})\n",
    "\n",
    "# Drop rows where the application date is missing, as it's crucial for temporal split\n",
    "accepted_final.dropna(subset=['application_date'], inplace=True)\n",
    "rejected_final.dropna(subset=['application_date'], inplace=True)\n",
    "\n",
    "# --- Step 7: Handle Missing Values ---\n",
    "print(\"Handling missing values...\")\n",
    "print(\"Missing values before imputation:\")\n",
    "print(accepted_final.isnull().sum())\n",
    "print(rejected_final.isnull().sum())\n",
    "print(\"Dropping null\")\n",
    "accepted_final = accepted_final.dropna()\n",
    "rejected_final = rejected_final.dropna()\n",
    "# print(rejected_final.isnull().sum())\n",
    "\n",
    "print(\"Splitting data by time period...\")\n",
    "accepted_final['year'] = accepted_final['application_date'].dt.year\n",
    "rejected_final['year'] = rejected_final['application_date'].dt.year\n",
    "\n",
    "recession_years = range(2007, 2013) # 2007-2012 inclusive\n",
    "post_recession_years = range(2013, 2019) # 2013-2018 inclusive\n",
    "\n",
    "accepted_final_recession_df = accepted_final[accepted_final['year'].isin(recession_years)].copy()\n",
    "accepted_final_post_recession_df = accepted_final[accepted_final['year'].isin(post_recession_years)].copy()\n",
    "\n",
    "rejected_final_recession_df = rejected_final[rejected_final['year'].isin(recession_years)].copy()\n",
    "rejected_final_post_recession_df = rejected_final[rejected_final['year'].isin(post_recession_years)].copy()\n",
    "\n",
    "# Drop the date and year columns as they are no longer needed for modeling itself\n",
    "accepted_final_recession_df.drop(columns=['application_date'], inplace=True)\n",
    "accepted_final_post_recession_df.drop(columns=['application_date'], inplace=True)\n",
    "rejected_final_recession_df.drop(columns=['application_date'], inplace=True)\n",
    "rejected_final_post_recession_df.drop(columns=['application_date'], inplace=True)\n",
    "\n",
    "print(f\"\\nRecession Era (2007-2012) dataset shape:\\n -> Accepted: {len(accepted_final_recession_df)}\\n -> Rejected: {len(rejected_final_recession_df)}\")\n",
    "print(f\"\\nPost-Recession Era (2013-2018) dataset shape:\\n -> Accepted: {len(accepted_final_post_recession_df)}\\n -> Rejected: {len(rejected_final_post_recession_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting all dataframes to length '92865'\n",
      "\n",
      "Recession Era (2007-2012) dataset shape:\n",
      " -> Accepted: 92865\n",
      " -> Rejected: 92865\n",
      "\n",
      "Post-Recession Era (2013-2018) dataset shape:\n",
      " -> Accepted: 92865\n",
      " -> Rejected: 92865\n"
     ]
    }
   ],
   "source": [
    "max_rows = min(len(accepted_final_recession_df), len(accepted_final_post_recession_df), len(rejected_final_recession_df), len(rejected_final_post_recession_df))\n",
    "print(f\"Cutting all dataframes to length '{max_rows}'\")\n",
    "random_shuffle_seed = 42\n",
    "accepted_final_recession_df = accepted_final_recession_df.sample(frac=1, random_state=random_shuffle_seed)\n",
    "accepted_final_post_recession_df = accepted_final_post_recession_df.sample(frac=1, random_state=random_shuffle_seed)\n",
    "rejected_final_recession_df = rejected_final_recession_df.sample(frac=1, random_state=random_shuffle_seed)\n",
    "rejected_final_post_recession_df = rejected_final_post_recession_df.sample(frac=1, random_state=random_shuffle_seed)\n",
    "\n",
    "accepted_final_recession_df = accepted_final_recession_df.head(max_rows)\n",
    "accepted_final_post_recession_df = accepted_final_post_recession_df.head(max_rows)\n",
    "rejected_final_recession_df = rejected_final_recession_df.head(max_rows)\n",
    "rejected_final_post_recession_df = rejected_final_post_recession_df.head(max_rows)\n",
    "\n",
    "print(f\"\\nRecession Era (2007-2012) dataset shape:\\n -> Accepted: {len(accepted_final_recession_df)}\\n -> Rejected: {len(rejected_final_recession_df)}\")\n",
    "print(f\"\\nPost-Recession Era (2013-2018) dataset shape:\\n -> Accepted: {len(accepted_final_post_recession_df)}\\n -> Rejected: {len(rejected_final_post_recession_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining datasets...\n",
      "Combined dataset shape: (371460, 8)\n",
      "[21:34:25] Starting preprocessing...\n",
      "[21:34:25] Found 7880 unique purposes\n",
      "[21:34:25] Loading model...\n",
      "[21:34:34] Encoding 7880 purposes (batch_size=512)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [00:01<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:34:35] Embeddings generated: (7880, 384)\n",
      "[21:34:35] Creating embedding dictionary...\n",
      "[21:34:35] Mapping embeddings to 371460 rows...\n",
      "[21:34:36] Missing embeddings: 0 rows\n",
      "[21:34:36] Creating embedding columns...\n",
      "[21:34:36] Embedding columns created: (371460, 384)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Combine DataFrames ---\n",
    "print(\"Combining datasets...\")\n",
    "combined_df = pd.concat([accepted_final_recession_df, accepted_final_post_recession_df, rejected_final_recession_df, rejected_final_post_recession_df], ignore_index=True)\n",
    "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "\n",
    "# --- 1. Preprocessing ---\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting preprocessing...\")\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "unique_purposes = combined_df['purpose'].unique()\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Found {len(unique_purposes)} unique purposes\")\n",
    "\n",
    "# --- 2. Generate Embeddings ---\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Loading model...\")\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
    "\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Encoding {len(unique_purposes)} purposes (batch_size=512)...\")\n",
    "unique_embeddings = model.encode(unique_purposes, \n",
    "                               batch_size=512, \n",
    "                               show_progress_bar=True, \n",
    "                               convert_to_numpy=True)\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Embeddings generated: {unique_embeddings.shape}\")\n",
    "\n",
    "# --- 3. Create Embedding Dictionary ---\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Creating embedding dictionary...\")\n",
    "embedding_dict = dict(zip(unique_purposes, unique_embeddings))\n",
    "\n",
    "# --- 4. Map Embeddings to DataFrame ---\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Mapping embeddings to {len(combined_df)} rows...\")\n",
    "combined_df['embedding_array'] = combined_df['purpose'].map(embedding_dict)\n",
    "\n",
    "# Check for missing embeddings\n",
    "missing = combined_df['embedding_array'].isna().sum()\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Missing embeddings: {missing} rows\")\n",
    "if missing > 0:\n",
    "    combined_df['embedding_array'] = combined_df['embedding_array'].apply(\n",
    "        lambda x: np.zeros(unique_embeddings.shape[1]) if x is None else x\n",
    "    )\n",
    "\n",
    "# --- 5. Convert to Columns ---\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Creating embedding columns...\")\n",
    "embedding_columns = pd.DataFrame(\n",
    "    np.vstack(combined_df['embedding_array']),\n",
    "    columns=[f'purpose_embed_{i}' for i in range(unique_embeddings.shape[1])],\n",
    "    index=combined_df.index\n",
    ")\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Embedding columns created: {embedding_columns.shape}\")\n",
    "combined_df = combined_df.drop(['purpose', 'embedding_array'], axis=1)\n",
    "combined_df = pd.merge(combined_df, embedding_columns, left_index=True, right_index=True)\n",
    "\n",
    "# One-hot encode 'addr_state'\n",
    "combined_df = pd.get_dummies(\n",
    "    combined_df, \n",
    "    columns=['addr_state'], \n",
    "    prefix='state', \n",
    "    dtype=np.int8  # Reduces memory usage by 75% vs float64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recession Era (2007-2012) dataset shape: (185730, 440)\n",
      "Post-Recession Era (2013-2018) dataset shape: (185730, 440)\n",
      "\n",
      "Preprocessing Complete.\n",
      "\n",
      "Recession Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 185730 entries, 0 to 278594\n",
      "Columns: 440 entries, dti to state_WY\n",
      "dtypes: float32(384), float64(4), int64(1), int8(51)\n",
      "memory usage: 289.6 MB\n",
      "\n",
      "Post-Recession Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 185730 entries, 92865 to 371459\n",
      "Columns: 440 entries, dti to state_WY\n",
      "dtypes: float32(384), float64(4), int64(1), int8(51)\n",
      "memory usage: 289.6 MB\n"
     ]
    }
   ],
   "source": [
    "# --- Step 9: Extract Year and Temporal Split ---\n",
    "# Define time periods\n",
    "recession_years = range(2007, 2013) # 2007-2012 inclusive\n",
    "post_recession_years = range(2013, 2019) # 2013-2018 inclusive\n",
    "\n",
    "recession_df = combined_df[combined_df['year'].isin(recession_years)].copy()\n",
    "post_recession_df = combined_df[combined_df['year'].isin(post_recession_years)].copy()\n",
    "\n",
    "# Drop the date and year columns as they are no longer needed for modeling itself\n",
    "recession_df.drop(columns=['year'], inplace=True)\n",
    "post_recession_df.drop(columns=['year'], inplace=True)\n",
    "\n",
    "print(f\"Recession Era (2007-2012) dataset shape: {recession_df.shape}\")\n",
    "print(f\"Post-Recession Era (2013-2018) dataset shape: {post_recession_df.shape}\")\n",
    "\n",
    "# --- Final Check ---\n",
    "print(\"\\nPreprocessing Complete.\")\n",
    "print(\"\\nRecession Data Info:\")\n",
    "recession_df.info()\n",
    "print(\"\\nPost-Recession Data Info:\")\n",
    "post_recession_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 185730 entries, 0 to 278594\n",
      "Columns: 440 entries, dti to state_WY\n",
      "dtypes: float32(384), float64(4), int64(1), int8(51)\n",
      "memory usage: 289.6 MB\n",
      "\n",
      "Target variable 'status' unique values: [1 0]\n",
      "\n",
      "Encoded target classes: [0 1]\n",
      "\n",
      "Training set shape: X=(130011, 439), y=(130011,)\n",
      "Testing set shape: X=(55719, 439), y=(55719,)\n",
      "\n",
      "--- Training Random Forest ---\n",
      "Random Forest training complete.\n",
      "\n",
      "--- Training Bagging Classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier training complete.\n",
      "\n",
      "--- Training XGBoost Classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cedergrund/.pyenv/versions/3.11.4/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [22:04:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost training complete.\n",
      "\n",
      "Using string class names for report: ['0', '1']\n",
      "\n",
      "--- Model Evaluation ---\n",
      "\n",
      "--- Random Forest Results ---\n",
      "Accuracy: 0.9427\n",
      "Precision (binary avg): 0.9341\n",
      "Recall (binary avg): 0.9527\n",
      "F1-Score (binary avg): 0.9433\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25987  1873]\n",
      " [ 1317 26542]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94     27860\n",
      "           1       0.93      0.95      0.94     27859\n",
      "\n",
      "    accuracy                           0.94     55719\n",
      "   macro avg       0.94      0.94      0.94     55719\n",
      "weighted avg       0.94      0.94      0.94     55719\n",
      "\n",
      "\n",
      "--- Bagging Results ---\n",
      "Accuracy: 0.9583\n",
      "Precision (binary avg): 0.9481\n",
      "Recall (binary avg): 0.9697\n",
      "F1-Score (binary avg): 0.9588\n",
      "\n",
      "Confusion Matrix:\n",
      "[[26382  1478]\n",
      " [  844 27015]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96     27860\n",
      "           1       0.95      0.97      0.96     27859\n",
      "\n",
      "    accuracy                           0.96     55719\n",
      "   macro avg       0.96      0.96      0.96     55719\n",
      "weighted avg       0.96      0.96      0.96     55719\n",
      "\n",
      "\n",
      "--- XGBoost Results ---\n",
      "Accuracy: 0.9471\n",
      "Precision (binary avg): 0.9297\n",
      "Recall (binary avg): 0.9674\n",
      "F1-Score (binary avg): 0.9482\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25823  2037]\n",
      " [  908 26951]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95     27860\n",
      "           1       0.93      0.97      0.95     27859\n",
      "\n",
      "    accuracy                           0.95     55719\n",
      "   macro avg       0.95      0.95      0.95     55719\n",
      "weighted avg       0.95      0.95      0.95     55719\n",
      "\n",
      "\n",
      "--- Best Performing Model (based on binary F1-Score) ---\n",
      "Model: Bagging\n",
      "F1-Score: 0.9588\n",
      "\n",
      "--- Random Forest Feature Importances (Top 10) ---\n",
      "fico_score           0.301128\n",
      "emp_length           0.300368\n",
      "dti                  0.163910\n",
      "loan_amnt            0.069492\n",
      "purpose_embed_368    0.006640\n",
      "purpose_embed_110    0.004896\n",
      "purpose_embed_100    0.003834\n",
      "purpose_embed_167    0.003243\n",
      "purpose_embed_376    0.003116\n",
      "purpose_embed_8      0.002821\n",
      "dtype: float64\n",
      "\n",
      "--- XGBoost Feature Importances (Top 10) ---\n",
      "emp_length           0.129305\n",
      "purpose_embed_368    0.066874\n",
      "purpose_embed_370    0.055051\n",
      "purpose_embed_134    0.052632\n",
      "purpose_embed_296    0.051190\n",
      "purpose_embed_219    0.045609\n",
      "purpose_embed_10     0.045218\n",
      "purpose_embed_383    0.037171\n",
      "purpose_embed_13     0.034925\n",
      "purpose_embed_29     0.033706\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np # Used for handling potential NaN/inf after scaling\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "# Assume 'recession_df' is your pre-loaded DataFrame\n",
    "# recession_df = pd.read_csv('your_data.csv') # Or however you load it\n",
    "\n",
    "print(\"Original DataFrame Info:\")\n",
    "recession_df.info()\n",
    "print(f\"\\nTarget variable 'status' unique values: {recession_df['status'].unique()}\")\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "target_column = 'status'\n",
    "X = recession_df.drop(target_column, axis=1)\n",
    "y = recession_df[target_column]\n",
    "\n",
    "# --- 2. Encode Target Variable ---\n",
    "# Models require numerical target variables. If 'status' is categorical (e.g., strings),\n",
    "# encode it into integers.\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(f\"\\nEncoded target classes: {le.classes_}\") # Shows mapping: 0 -> class1, 1 -> class2, etc.\n",
    "\n",
    "# --- 3. Split Data into Training and Testing Sets ---\n",
    "# Use stratify=y_encoded to maintain class proportions in train/test splits,\n",
    "# which is important for classification, especially if classes are imbalanced.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.3,        # 30% for testing, 70% for training\n",
    "    random_state=42,      # For reproducibility\n",
    "    stratify=y_encoded    # Keep class distribution consistent\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Testing set shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "\n",
    "# --- 4. Scale Features ---\n",
    "# Although tree-based models aren't strictly sensitive to feature scaling,\n",
    "# it's good practice and can sometimes help, especially with regularization (like in XGBoost).\n",
    "# Important: Fit the scaler ONLY on the training data, then transform both train and test data.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Handle potential NaNs or Infs that might arise from scaling (if columns had zero variance)\n",
    "X_train_scaled = np.nan_to_num(X_train_scaled)\n",
    "X_test_scaled = np.nan_to_num(X_test_scaled)\n",
    "\n",
    "# Convert scaled arrays back to DataFrames (optional, but can be helpful for inspection)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "\n",
    "# --- 5. Define and Train Models ---\n",
    "\n",
    "# --- Random Forest ---\n",
    "print(\"\\n--- Training Random Forest ---\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,     # Number of trees in the forest (common starting point)\n",
    "    random_state=42,      # For reproducibility\n",
    "    n_jobs=-1,            # Use all available CPU cores\n",
    "    class_weight='balanced' # Useful if classes are imbalanced\n",
    ")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "print(\"Random Forest training complete.\")\n",
    "\n",
    "# --- Bagging (Bootstrap Aggregating) ---\n",
    "# Often used with Decision Trees as the base estimator (default if None is specified)\n",
    "print(\"\\n--- Training Bagging Classifier ---\")\n",
    "bagging_model = BaggingClassifier(\n",
    "    # base_estimator=DecisionTreeClassifier(), # Default if None\n",
    "    n_estimators=50,      # Number of base estimators (trees)\n",
    "    random_state=42,      # For reproducibility\n",
    "    n_jobs=-1             # Use all available CPU cores\n",
    ")\n",
    "bagging_model.fit(X_train_scaled, y_train)\n",
    "print(\"Bagging Classifier training complete.\")\n",
    "\n",
    "\n",
    "# --- XGBoost ---\n",
    "print(\"\\n--- Training XGBoost Classifier ---\")\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,     # Number of boosting rounds (trees)\n",
    "    learning_rate=0.1,    # Step size shrinkage to prevent overfitting\n",
    "    max_depth=3,          # Maximum depth of a tree (common starting point)\n",
    "    random_state=42,      # For reproducibility\n",
    "    n_jobs=-1,            # Use all available CPU cores\n",
    "    use_label_encoder=False, # Recommended to avoid deprecation warnings\n",
    "    eval_metric='logloss' # Common evaluation metric for binary/multi-class classification\n",
    "    # Add 'scale_pos_weight' here if dealing with imbalanced classes, e.g.,\n",
    "    # scale_pos_weight = sum(y_train == 0) / sum(y_train == 1) # For binary\n",
    ")\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "print(\"XGBoost training complete.\")\n",
    "\n",
    "\n",
    "# --- 6. Make Predictions on Test Set ---\n",
    "\n",
    "rf_preds = rf_model.predict(X_test_scaled)\n",
    "bagging_preds = bagging_model.predict(X_test_scaled)\n",
    "xgb_preds = xgb_model.predict(X_test_scaled)\n",
    "# --- 7. Evaluate Models ---\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": rf_preds,\n",
    "    \"Bagging\": bagging_preds,\n",
    "    \"XGBoost\": xgb_preds\n",
    "}\n",
    "\n",
    "# Get the original class labels detected by the encoder\n",
    "# These might be numbers (e.g., array([0, 1])) if your original 'status' was numeric\n",
    "class_labels = le.classes_\n",
    "\n",
    "# *** FIX: Convert class labels to strings for the report ***\n",
    "# The classification_report expects string names in target_names.\n",
    "class_names_str = [str(c) for c in class_labels]\n",
    "print(f\"\\nUsing string class names for report: {class_names_str}\") # Debug print\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "\n",
    "for model_name, y_pred in models.items():\n",
    "    print(f\"\\n--- {model_name} Results ---\")\n",
    "\n",
    "    # Calculate Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # Determine averaging strategy based on the number of unique classes\n",
    "    num_classes = len(class_labels)\n",
    "    avg_strategy = 'weighted' if num_classes > 2 else 'binary'\n",
    "    # Determine positive label ONLY for binary case (usually label 1)\n",
    "    # Check if 1 exists in the encoded labels before assigning\n",
    "    pos_label = 1 if avg_strategy == 'binary' and 1 in y_encoded else None\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average=avg_strategy, pos_label=pos_label, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=avg_strategy, pos_label=pos_label, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average=avg_strategy, pos_label=pos_label, zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision ({avg_strategy} avg): {precision:.4f}\")\n",
    "    print(f\"Recall ({avg_strategy} avg): {recall:.4f}\")\n",
    "    print(f\"F1-Score ({avg_strategy} avg): {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    # Pass the actual numerical labels for the confusion matrix if needed for specific order\n",
    "    print(confusion_matrix(y_test, y_pred, labels=np.unique(y_encoded))) # Ensure order if necessary\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    # *** Use the STRING version of class names for target_names ***\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names_str, zero_division=0))\n",
    "\n",
    "# --- 8. Determine Best Performance (Based on your criteria) ---\n",
    "# (Rest of the code for comparison remains the same, using the calculated f1_scores dict)\n",
    "# Make sure the f1_score calculation here also uses the correct avg_strategy and pos_label\n",
    "f1_scores = {\n",
    "    \"Random Forest\": f1_score(y_test, rf_preds, average=avg_strategy, pos_label=pos_label, zero_division=0),\n",
    "    \"Bagging\": f1_score(y_test, bagging_preds, average=avg_strategy, pos_label=pos_label, zero_division=0),\n",
    "    \"XGBoost\": f1_score(y_test, xgb_preds, average=avg_strategy, pos_label=pos_label, zero_division=0)\n",
    "}\n",
    "\n",
    "best_model_name = max(f1_scores, key=f1_scores.get)\n",
    "print(f\"\\n--- Best Performing Model (based on {avg_strategy} F1-Score) ---\")\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"F1-Score: {f1_scores[best_model_name]:.4f}\")\n",
    "\n",
    "# --- Optional: Feature Importance (for tree-based models) ---\n",
    "# Helps understand which features are most influential\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    print(\"\\n--- Random Forest Feature Importances (Top 10) ---\")\n",
    "    rf_importances = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
    "    print(rf_importances.nlargest(10))\n",
    "\n",
    "if hasattr(xgb_model, 'feature_importances_'):\n",
    "    print(\"\\n--- XGBoost Feature Importances (Top 10) ---\")\n",
    "    xgb_importances = pd.Series(xgb_model.feature_importances_, index=X.columns)\n",
    "    print(xgb_importances.nlargest(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
